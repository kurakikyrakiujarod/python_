# 爬虫

播放地址 https://www.bilibili.com/video/av22571713/

## Urllib库实战

爬取豆瓣出版社列表并写入文件中（https://read.douban.com/provider/all）

```python
import re
import urllib.request

data = urllib.request.urlopen("https://read.douban.com/provider/all").read()
data = data.decode("utf-8")
pat = '<div class="name">(.*?)</div>'
mydata = re.compile(pat).findall(data)
f = open("./1.txt", "w")
for i in mydata:
    f.write(i)
    print(i)
    f.write("\n")
f.close()
```

```python
urllib.request.urlretrieve("https://www.hellobi.com/", filename="1.html")  # 拿网页
urllib.request.urlcleanup()  # 清除缓存
```

```python
file = urllib.request.urlopen("https://read.douban.com/provider/all")
file.info()  # <http.client.HTTPMessage at 0x1e661a0c390>
file.getcode() # 200
file.geturl() # 'https://read.douban.com/provider/all'
```

```python
urllib.request.urlopen("https://www.hellobi.com/", timeout=1) # 超时设置 1秒
```

```python
from fake_useragent import FakeUserAgentError
from fake_useragent import UserAgent

try:
    ua = UserAgent()
except FakeUserAgentError:
    pass
```

```python
#coding:utf8
import datetime
import time
 
def doSth():
    # 把爬虫程序放在这个类里
    print(u'这个程序要开始疯狂的运转啦')
 
# 一般网站都是1:00点更新数据，所以每天凌晨一点启动
def main(h=1,m=0):
    while True:
        now = datetime.datetime.now()
        # print(now.hour, now.minute)
        if now.hour == h and now.minute == m:
            doSth()
        # 每隔60秒检测一次
        time.sleep(60)
    
 
main()
```

代理

```python
import socket
import socks
import requests

socks.set_default_proxy(socks.SOCKS5, "127.0.0.1", 1080)
socket.socket = socks.socksocket
print(requests.get('http://ifconfig.me/ip').text)
```

 Python Extension Packages <https://www.lfd.uci.edu/~gohlke/pythonlibs/>

