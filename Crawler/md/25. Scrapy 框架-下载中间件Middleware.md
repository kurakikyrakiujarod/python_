

### 2. 激活一个下载DOWNLOADER_MIDDLEWARES
要激活一个下载器中间件组件，将其添加到 `DOWNLOADER_MIDDLEWARES`设置中，该设置是一个字典，其键是中间件类路径，它们的值是中间件命令
```python
DOWNLOADER_MIDDLEWARES  =  { 
    'myproject.middlewares.CustomDownloaderMiddleware' ： 543 ，
}
```
如果要禁用内置中间件（`DOWNLOADER_MIDDLEWARES_BASE`默认情况下已定义和启用的中间件 ），则必须在项目`DOWNLOADER_MIDDLEWARES`设置中定义它，并将“ 无” 作为其值。例如，如果您要禁用用户代理中间件

```python
DOWNLOADER_MIDDLEWARES  =  { 
    'myproject.middlewares.CustomDownloaderMiddleware' ： 543 ，
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware' ： None ，
}
```
最后，请记住，某些中间件可能需要通过特定设置启用。

### 3. 编写你自己的下载中间件
每个中间件组件都是一个Python类，它定义了一个或多个以下方法
```python
class scrapy.downloadermiddlewares.DownloaderMiddleware
```
#### 3.1 process_request(self, request, spider)
process_request()必须返回其中之一
- 返回 None
- 返回一个 Response 对象
- 返回一个 Request 对象
- raise IgnoreRequest
参数:
- request (Request 对象) – 处理的request
- spider (Spider 对象) – 该request对应的spider
#### 3.2 process_response(self, request, response, spider)


- process_request() 必须返回以下其中之一: 

    - 返回一个 Response 对象、 
    - 返回一个 Request 对象
    - 或raise一个 IgnoreRequest 异常
- 参数:
    - request (Request 对象) – response所对应的request
    - response (Response 对象) – 被处理的response
    - spider (Spider 对象) – response所对应的spider


### 4 使用代理
settings.py
```python
PROXIES=[
    {"ip":"122.236.158.78:8118"},
    {"ip":"112.245.78.90:8118"}
]
DOWNLOADER_MIDDLEWARES = {
    #'xiaoshuo.middlewares.XiaoshuoDownloaderMiddleware': 543,
    'xiaoshuo.proxyMidde.ProxyMidde':100
}
```
创建一个midderwares
```python
from xiaoshuo.settings import PROXIES
import random
class ProxyMidde(object):
    def process_request(self, request, spider):
            proxy = random.choice(PROXIES)
            request.meta['proxy']='http://'+proxy['ip']
```
写一个spider测试
```python
from scrapy import Spider


class ProxyIp(Spider):
    name = 'ip'
    #http://www.882667.com/
    start_urls = ['http://ip.cn']

    def parse(self, response):
        print(response.text)
```

### 5 使用动态UA

```python
# 随机的User-Agent
class RandomUserAgent(object):
    def process_request(self, request, spider):
        useragent = random.choice(USER_AGENTS)
        request.headers.setdefault("User-Agent", useragent)
```