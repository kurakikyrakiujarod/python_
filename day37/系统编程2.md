# 系统编程

<a href="https://www.cnblogs.com/xiao987334176/p/9025072.html">python 全栈开发，Day39(进程同步控制(锁,信号量,事件),进程间通信(队列,生产者消费者模型))</a>

## 进程同步

## 锁 

**多进程抢占输出资源**

```python
import os
import time
import random
from multiprocessing import Process


def work(n):
    print('%s: %s is running' % (n, os.getpid()))
    time.sleep(random.random())
    print('%s:%s is done' % (n, os.getpid()))


if __name__ == '__main__':
    for i in range(3):
        p = Process(target=work, args=(i,))
        p.start()

# 1: 12016 is running
# 0: 6504 is running
# 2: 8076 is running
# 2:8076 is done
# 0:6504 is done
# 1:12016 is done
```

**用锁**

```python
import os
import time
import random
from multiprocessing import Lock
from multiprocessing import Process


def work(n, l):
    l.acquire()  # 取得锁
    print('%s: %s is running' % (n, os.getpid()))
    time.sleep(random.random())
    print('%s:%s is done' % (n, os.getpid()))
    l.release()  # 释放锁


if __name__ == '__main__':
    lock = Lock()  # 创建锁
    for i in range(5):
        p = Process(target=work, args=(i, lock))
        p.start()
# 2: 5608 is running
# 2:5608 is done
# 3: 7808 is running
# 3:7808 is done
# 0: 8624 is running
# 0:8624 is done
# 1: 5968 is running
# 1:5968 is done
# 4: 4520 is running
# 4:4520 is done
```



理论上来讲，进程一般是异步的，但是加了锁之后，就变成同步了。那么谁先拿到钥匙呢？满足以下2个条件：
1.操作系统先响应的进程 2.当时没有时间片轮询，刚好就是它

```
lock.acquire()和lock.release()之间的代码，表示被锁住了。
```

```python
import os
import time
import random
from multiprocessing import Process
 
def work(n):
    print('%s: %s is running' %(n,os.getpid()))
    time.sleep(random.random())
    print('%s:%s is done' %(n,os.getpid()))
 
if __name__ == '__main__':
    for i in range(5):
        p=Process(target=work,args=(i,))
        p.start()
        p.join()
# 0: 10836 is running
# 0:10836 is done
# 1: 9088 is running
# 1:9088 is done
# 2: 8624 is running
# 2:8624 is done
# 3: 4900 is running
# 3:4900 is done
# 4: 1616 is running
# 4:1616 is done
```

如果锁加在进程的开始和结束，参考work函数，它是同步的。join也是同步的，但是，他们之间的区别在于，锁执行时是无序的，join是有序的。由并发变成了串行，牺牲了运行效率，但避免了竞争。

同步控制：只要用到了锁，锁之间的代码就会变成同步的；锁：控制一段代码，同一时间 只能被一个进程执行。

## 查票

ticket文件

```
{"count":1}
```

```python
import json
from multiprocessing import Process


def check_ticket(i):
    with open('ticket') as f:
        ticket_count = json.load(f)
    print('person%s' % i, ticket_count['count'])


if __name__ == '__main__':
    for i in range(5):
        Process(target=check_ticket, args=(i,)).start()

# person0 1
# person3 1
# person2 1
# person1 1
# person4 1
```

## 购票

有错

```python
import json
import time
import random
from multiprocessing import Process


def buy_ticket(i):
    with open('ticket') as f:  # 读取文件
        tick_count = json.load(f)  # 反序列化
        time.sleep(random.random())  # 模拟延时
    if tick_count['count'] > 0:  # 当余票小于0时
        print('person%s购票成功' % i)
        tick_count['count'] -= 1  # 票数减1
    else:
        print('余票不足,person%s购票失败' % i)
    time.sleep(random.random())  # 写入延时
    with open('ticket', 'w') as f:
        json.dump(tick_count, f)  # 写入文件


if __name__ == '__main__':
    for i in range(5):
        Process(target=buy_ticket, args=(i,)).start()

# person4购票成功
# person2购票成功
# person1购票成功
# person3购票成功
# person0购票成功
```

加锁

```python
import json
import time
import random
from multiprocessing import Process, Lock


def buy_ticket(i, l):  # 购票
    l.acquire()  # 取得锁
    with open('ticket') as f:  # 读取文件
        tick_count = json.load(f)  # 反序列化
        time.sleep(random.random())  # 读取延时
    if tick_count['count'] > 0:  # 当余票大于于0时
        print('person%s购票成功' % i)
        tick_count['count'] -= 1  # 票数减1
    else:
        print('余票不足,person%s购票失败' % i)
    time.sleep(random.random())  # 写入延时
    with open('ticket', 'w') as f:
        json.dump(tick_count, f)  # 写入文件
    l.release()  # 释放锁


if __name__ == '__main__':
    lock = Lock()  # 创建锁
    for i in range(5):  # 模拟5个用户抢票
        Process(target=buy_ticket, args=(i, lock)).start()
```

ticket

```
{"count": 1}
```

执行结果

```
person2购票成功
余票不足,person0购票失败
余票不足,person4购票失败
余票不足,person1购票失败
余票不足,person3购票失败
```

加锁可以保证多个进程修改同一块数据时，同一时间只能有一个任务可以进行修改，即串行的修改，没错，速度是慢了，但牺牲了速度却保证了数据安全。虽然可以用文件共享数据实现进程间通信，但问题是：1.效率低（共享数据基于文件，而文件是硬盘上的数据）；2.需要自己加锁处理。

因此我们最好找寻一种解决方案能够兼顾：1、效率高（多个进程共享一块内存的数据）；2、帮我们处理好锁问题。这就是 mutiprocessing模块为我们提供的基于消息的IPC通信机制：队列和管道。队列和管道都是将数据存放于内存中。队列又是基于（管道+锁）实现的，可以让我们从复杂的锁问题中解脱出来，我们应该尽量避免使用共享数据，尽可能使用消息传递和队列，避免处理复杂的同步和锁问题，而且在进程数目增多时，往往可以获得更好的可获展性。

## 信号量 

互斥锁同时只允许一个线程更改数据，而信号量Semaphore是同时允许一定数量的线程更改数据。

假设商场里有4个迷你唱吧，所以同时可以进去4个人，如果来了第五个人就要在外面等待，等到有人出来才能再进去玩。实现：信号量同步基于内部计数器，每调用一次acquire()，计数器减1；每调用一次release()，计数器加1。当计数器为0时，acquire()调用被阻塞。信号量同步机制适用于访问像服务器这样的有限资源。信号量与进程池的概念很像，但是要区分开，信号量涉及到加锁的。

## KTV

```python
import time
import random
from multiprocessing import Process, Semaphore


def ktv(p, s):
    s.acquire()  # 取得锁
    print('person %s 进来唱歌了' % p)
    time.sleep(random.random())
    print('person %s 从ktv出去了' % p)
    s.release()  # 释放锁


if __name__ == '__main__':
    sem = Semaphore(4)  # 初始化信号量，数量为4
    for i in range(6):  # 模拟6个人
        Process(target=ktv, args=(i, sem)).start()

# person 5 进来唱歌了
# person 0 进来唱歌了
# person 1 进来唱歌了
# person 1 从ktv出去了
# person 4 进来唱歌了
# person 3 从ktv出去了
# person 2 进来唱歌了
# person 0 从ktv出去了
# person 5 从ktv出去了
# person 4 从ktv出去了
# person 2 从ktv出去了
```

## 事件

事件介绍

```
python线程的事件用于主线程控制其他线程的执行，事件主要提供了三个方法 set、wait、clear。
事件处理的机制：
全局定义了一个“Flag”，如果“Flag”值为False，那么当程序执行event.wait方法时就会阻塞，如果“Flag”值为True，那么event.wait方法时便不再阻塞。
clear：将“Flag”设置为False
set：将“Flag”设置为True
is_set：判断当前的状态是否为True
```

```python
from multiprocessing import Event

e = Event()  # 创建一个事件的对象
print(e.is_set())  # 在事件的创始之初,状态为False
```

```python
from multiprocessing import Event

e = Event()  # 创建一个事件的对象
print(e.is_set())  # 在事件的创世之初,状态为False
e.wait()
print('1')
# 执行输出： False，然后程序一致卡着
```

```python
from multiprocessing import Event

e = Event()  # 创建一个事件的对象
print(e.is_set())  # 在事件的创始之初,状态为False
e.set()  # 将状态设置为True
e.wait()
print(e.is_set())  # 查看状态
print('1')
# False
# True
# 1
```

## 红绿灯

模拟红绿灯

```python
import time


def traffic_light():
    while True:
        print('\033[1;31m红灯亮\033[0m')
        time.sleep(1)
        print('\033[1;32m绿灯亮\033[0m')
        time.sleep(1)
        break


traffic_light()
# 红灯亮
# 绿灯亮
```

下面来创建车，加一个红绿灯进程。

```python
import time
from multiprocessing import Process, Event


def traffic_light():
    while True:
        print('\033[1;31m红灯亮\033[0m')
        time.sleep(2)
        print('\033[1;32m绿灯亮\033[0m')
        time.sleep(2)
        break


def car(i):
    print('car%s通过路口' % i)


if __name__ == '__main__':
    Process(target=traffic_light).start()
    for i in range(1, 6):
        Process(target=car, args=(i,)).start()

# car4通过路口
# car1通过路口
# 红灯亮
# car3通过路口
# car5通过路口
# car2通过路口
# 绿灯亮
```

```python
import time

flag = False

while True:
    if flag is True:
        flag = False
        print(0)
    else:
        flag = True
        print(1)
    time.sleep(1)
```

最终版

```python
import time
import random
from multiprocessing import Process, Event


def traffic_light(e):
    count = 0
    while count < 6:
        print('\033[1;31m红灯亮\033[0m')
        time.sleep(2)
        if not e.is_set(): e.set()
        print('\033[1;32m绿灯亮\033[0m')
        time.sleep(2)
        if e.is_set(): e.clear()
        count += 1


def car(i, e):
    if not e.is_set():
        print('car%s正在等待' % i)
    e.wait()
    print('car%s通过路口' % i)


if __name__ == '__main__':
    e = Event()
    Process(target=traffic_light, args=(e,)).start()
    for i in range(1, 6):
        time.sleep(random.randint(1, 3))
        Process(target=car, args=(i, e)).start()

# 执行输出
# 红灯亮
# car1正在等待
# 绿灯亮
# car1通过路口
# car2通过路口
# 红灯亮
# 绿灯亮
# car3通过路口
# 红灯亮
# car4正在等待
# 绿灯亮
# car4通过路口
# car5通过路口
# 红灯亮
# 绿灯亮
# 红灯亮
# 绿灯亮
# 红灯亮
# 绿灯亮
```

## 进程间通信

## 队列

**概念介绍**

```python
import queue # 它能维护一个先进先出的秩序，它不能进行IPC
from multiprocessing import Queue,Process # 能维护一个先进先出的秩序，也能进行IPC
```

```
Queue([maxsize]) 创建共享的进程队列。
参数：maxsize是队列中允许的最大项数。如果省略此参数，则无大小限制。底层队列使用管道和锁定实现。另外，还需要运行支持线程以便队列中的数据传输到底层管道中。
```

**Queue的实例q具有以下方法**

```python
q.get( [ block [ ,timeout ] ] )
返回q中的一个项目。如果q为空，此方法将阻塞，直到队列中有项目可用为止。
block用于控制阻塞行为，默认为True。如果设置为False，将引发Queue.Empty异常（定义在Queue模块中）。
timeout是可选超时时间，用在阻塞模式中。如果在制定的时间间隔内没有项目变为可用，将引发Queue.Empty异常。

q.get_nowait( )
同q.get(False)方法。

q.put(item [, block [,timeout ] ] )
将item放入队列。如果队列已满，此方法将阻塞至有空间可用为止。
block控制阻塞行为，默认为True。如果设置为False，将引发Queue.Empty异常（定义在Queue库模块中）。
timeout指定在阻塞模式中等待可用空间的时间长短。超时后将引发Queue.Full异常。

q.qsize()
返回队列中目前项目的正确数量。
此函数的结果并不可靠，因为在返回结果和在稍后程序中使用结果之间，队列中可能添加或删除了项目。
在某些系统上，此方法可能引发NotImplementedError异常。

q.empty()
如果调用此方法时q为空，返回True。如果其他进程或线程正在往队列中添加项目，结果是不可靠的。
也就是说，在返回和使用结果之间，队列中可能已经加入新的项目

q.full()
如果q已满，返回为True。由于线程的存在，结果也可能是不可靠的（参考q.empty（）方法）。

其他方法(了解）
q.close()
关闭队列，防止队列中加入更多数据。
调用此方法时，后台线程将继续写入那些已入队列但尚未写入的数据，但将在此方法完成时马上关闭。
如果q被垃圾收集，将自动调用此方法。关闭队列不会在队列使用者中生成任何类型的数据结束信号或异常。
例如，如果某个使用者正被阻塞在get()操作上，关闭生产者中的队列不会导致get()方法返回错误。

q.cancel_join_thread()
不会在进程退出时自动连接后台线程。这可以防止join_thread()方法阻塞。

q.join_thread()
连接队列的后台线程。此方法用于在调用q.close()方法后，等待所有队列项被消耗。
默认情况下，此方法由不是q的原始创建者的所有进程调用。
调用q.cancel_join_thread()方法可以禁止这种行为。
```

```python
from multiprocessing import Queue

q = Queue()  # 创建共享的进程队列
q.put(1)  # 将一个值放入队列
q.put(2)
q.put('aaa')
print(q.get())  # 返回q中的一个项目
# 1
```

```python
import os
from multiprocessing import Process, Queue


def wahaha(q):
    print(q.get(), os.getpid(), os.getppid())


if __name__ == '__main__':
    q = Queue()  # 创建共享的进程队列
    Process(target=wahaha, args=(q,)).start()
    q.put(1)
    print(os.getpid())
# 9732
# 1 184 9732
```

**双向通信**既能取值，也能增加值

```python
import time
from multiprocessing import Process, Queue


def wahaha(q):
    print(q.get())  # 取值
    q.put('aaa')  # 增加一个aaa


if __name__ == '__main__':
    q = Queue()  # 创建共享的进程队列
    Process(target=wahaha, args=(q,)).start()
    # time.sleep(1)
    q.put(1)
    time.sleep(0.5)  # 等待0.5秒,让子进程执行完
    print(q.get())  # 取值
# 1
# aaa
```

**批量生产数据放入队列再批量获取结果**

```python
import os
import time
from multiprocessing import Queue, Process


# 输入
def inp_que(q):
    info = str(os.getpid())+'(put):'+time.asctime()
    q.put(info)
    print(info)


# 获取消息
def get_que(q):
    info = q.get()
    print('\033[32m{}(get):{}\033[0m'.format(os.getpid(), info))


if __name__ == '__main__':
    q = Queue(3)
    inp_list = []  # store input processes
    get_list = []  # store output processes

    # 输入进程
    for i in range(10):
        p = Process(target=inp_que, args=(q,))
        inp_list.append(p)
        p.start()

    # 输出进程
    for i in range(10):
        p = Process(target=get_que, args=(q,))
        get_list.append(p)
        p.start()

    for p in inp_list:
        p.join()
    for p in get_list:
        p.join()

# 11908(put):Fri Jan 25 13:26:42 2019
# 3264(get):11908(put):Fri Jan 25 13:26:42 2019
# 10448(put):Fri Jan 25 13:26:42 2019
# 11736(get):10448(put):Fri Jan 25 13:26:42 2019
# 2464(put):Fri Jan 25 13:26:42 2019
# 12220(get):2464(put):Fri Jan 25 13:26:42 2019
# 9360(put):Fri Jan 25 13:26:42 2019
# 4516(get):9360(put):Fri Jan 25 13:26:42 2019
# 11120(put):Fri Jan 25 13:26:42 2019
# 11592(get):11120(put):Fri Jan 25 13:26:42 2019
# 9876(put):Fri Jan 25 13:26:42 2019
# 9640(get):9876(put):Fri Jan 25 13:26:42 2019
# 3696(put):Fri Jan 25 13:26:42 2019
# 11036(get):3696(put):Fri Jan 25 13:26:42 2019
# 2584(put):Fri Jan 25 13:26:42 2019
# 11836(get):2584(put):Fri Jan 25 13:26:42 2019
# 3908(put):Fri Jan 25 13:26:42 2019
# 11912(get):3908(put):Fri Jan 25 13:26:42 2019
# 12076(put):Fri Jan 25 13:26:43 2019
# 6548(get):12076(put):Fri Jan 25 13:26:43 2019
```

## 生产者消费者模型

在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度。

**什么是生产者消费者模式**

生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯。所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取。阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。

```python
import time
import random
import os
from multiprocessing import Queue, Process


def consumer(q):
    while True:
        bao_zi = q.get()
        if bao_zi is None: break
        # if not bao_zi: break
        time.sleep(random.random())
        print('\033[1;32m{}消费了{}\033[0m'.format(os.getpid(), bao_zi))


def producer(q):
    for i in range(1, 11):
        time.sleep(random.random())
        bao_zi = '{}{}'.format('包子', i)
        q.put(bao_zi)
        print('\033[1;31m{}生产了{}'.format(os.getpid(), bao_zi))
    q.put(None)


if __name__ == '__main__':
    q = Queue()
    p = Process(target=producer, args=(q,))
    xxx = Process(target=consumer, args=(q,))
    p.start()
    xxx.start()

# 1172生产了包子1
# 1172生产了包子2
# 4720消费了包子1
# 1172生产了包子3
# 4720消费了包子2
# 4720消费了包子3
# 1172生产了包子4
# 1172生产了包子5
# 4720消费了包子4
# 1172生产了包子6
# 4720消费了包子5
# 1172生产了包子7
# 4720消费了包子6
# 1172生产了包子8
# 1172生产了包子9
# 4720消费了包子7
# 1172生产了包子10
# 4720消费了包子8
# 4720消费了包子9
# 4720消费了包子10
```

**主进程在生产者生产完毕后发送结束信号None**

结束信号None，不一定要由生产者发，主进程里同样可以发，但主进程需要等生产者结束后才应该发送该信号。

```python
import time
import random
import os
from multiprocessing import Queue, Process


def consumer(q):
    while True:
        bao_zi = q.get()
        if bao_zi is None: break
        time.sleep(random.random())
        print('\033[1;32m{}消费了{}\033[0m'.format(os.getpid(), bao_zi))


def producer(q):
    for i in range(1, 11):
        time.sleep(random.random())
        bao_zi = '{}{}'.format('包子', i)
        q.put(bao_zi)
        print('\033[1;31m{}生产了{}'.format(os.getpid(), bao_zi))


if __name__ == '__main__':
    q = Queue()
    p1 = Process(target=producer, args=(q,))
    p2 = Process(target=producer, args=(q,))
    c1 = Process(target=consumer, args=(q,))
    c2 = Process(target=consumer, args=(q,))
    p1.start()
    p2.start()
    c1.start()
    c2.start()

    # 等待两个生产者全部生产完毕
    p1.join()
    p2.join()
    # 发送结束信号到队列最后
    # 有几个消费者就应该发送几次结束信号None
    q.put(None)
    q.put(None)

# 执行输出
# 10200生产了包子1
# 9280生产了包子1
# 2476消费了包子1
# 4820消费了包子1
# 9280生产了包子2
# 9280生产了包子3
# 10200生产了包子2
# 10200生产了包子3
# 9280生产了包子4
# 4820消费了包子3
# 10200生产了包子4
# 2476消费了包子2
# 10200生产了包子5
# 9280生产了包子5
# 2476消费了包子3
# 4820消费了包子2
# 2476消费了包子4
# 2476消费了包子5
# 9280生产了包子6
# 10200生产了包子6
# 10200生产了包子7
# 9280生产了包子7
# 2476消费了包子5
# 4820消费了包子4
# 2476消费了包子6
# 10200生产了包子8
# 4820消费了包子6
# 9280生产了包子8
# 10200生产了包子9
# 9280生产了包子9
# 10200生产了包子10
# 4820消费了包子7
# 4820消费了包子8
# 9280生产了包子10
# 2476消费了包子7
# 2476消费了包子9
# 4820消费了包子8
# 4820消费了包子10
# 2476消费了包子9
# 4820消费了包子10
```

<a https://www.cnblogs.com/xiao987334176/p/9036763.html>python 全栈开发，Day40(进程间通信(队列和管道),进程间的数据共享Manager,进程池Pool)</a>

## JoinableQueue


创建可连接的共享进程队列。这就像是一个Queue对象，但队列允许项目的使用者通知生产者项目已经被成功处理。通知进程是使用共享的信号和条件变量来实现的。

方法介绍

```
JoinableQueue的实例p除了与Queue对象相同的方法之外，还具有以下方法：
q.task_done()
使用者使用此方法发出信号，表示q.get()返回的项目已经被处理。
如果调用此方法的次数大于从队列中删除的项目数量，将引发ValueError异常。
q.join()
生产者将使用此方法进行阻塞，直到队列中所有项目均被处理。
阻塞将持续到为队列中的每个项目均调用q.task_done()方法为止。
```

**JoinableQueue队列实现消费之生产者模型**

```python
import time
import random
from multiprocessing import Process, JoinableQueue


def producer(name, food, q):
    for i in range(1, 11):
        time.sleep(random.random())
        print('\033[1;31m{}生产{}{}\033[0m'.format(name, food, i))
        q.put('{}{}{}'.format(name, food, i))
    q.join()


def consumer(name, q):
    while True:
        get_info = q.get()
        time.sleep(random.random())
        print('\033[1;32m{}消费{}\033[0m'.format(name, get_info))
        q.task_done()


if __name__ == '__main__':
    q = JoinableQueue()
    p1 = Process(target=producer, args=('alex', 'jiaozi', q))
    p2 = Process(target=producer, args=('egon', 'bread', q))
    p1.start()
    p2.start()
    c1 = Process(target=consumer, args=('aoa', q))
    c2 = Process(target=consumer, args=('kuraki', q))
    c1.daemon = True
    c2.daemon = True
    c1.start()
    c2.start()
    p1.join()
    p2.join()
# alex生产jiaozi1
# egon生产bread1
# aoa消费egonbread1
# alex生产jiaozi2
# kuraki消费alexjiaozi1
# aoa消费alexjiaozi2
# egon生产bread2
# kuraki消费egonbread2
# alex生产jiaozi3
# egon生产bread3
# egon生产bread4
# alex生产jiaozi4
# alex生产jiaozi5
# egon生产bread5
# aoa消费alexjiaozi3
# kuraki消费egonbread3
# kuraki消费alexjiaozi4
# alex生产jiaozi6
# egon生产bread6
# alex生产jiaozi7
# aoa消费egonbread4
# kuraki消费alexjiaozi5
# egon生产bread7
# kuraki消费alexjiaozi6
# alex生产jiaozi8
# kuraki消费egonbread6
# egon生产bread8
# aoa消费egonbread5
# alex生产jiaozi9
# egon生产bread9
# alex生产jiaozi10
# kuraki消费alexjiaozi7
# aoa消费egonbread7
# kuraki消费alexjiaozi8
# egon生产bread10
# kuraki消费alexjiaozi9
# aoa消费egonbread8
# kuraki消费egonbread9
# aoa消费alexjiaozi10
# kuraki消费egonbread10
```

结束顺序：

- consumer中queue中的所有数据被消费
- producer join结束
- 主进程的代码结束
- consumer结束
- 主进程结束

## 管道

**pipe初使用**

```python
from multiprocessing import Process, Pipe


def f(conn):
    conn.send("Hello The_Third_Wave")
    conn.close()


if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())
    p.join()

# Hello The_Third_Wave
```

```python
from multiprocessing import Pipe

left, right = Pipe()
left.send('1234')
print(right.recv())
# 1234
```

应该特别注意管道端点的正确管理问题。如果是生产者或消费者中都没有使用管道的某个端点，就应将它关闭。
这也说明了为何在生产者中关闭了管道的输出端，在消费者中关闭管道的输入端。如果忘记执行这些步骤，程序可能在消费者中的recv（）操作上挂起。管道是由操作系统进行引用计数的，必须在所有进程中关闭管道后才能生成EOFError异常。因此，在生产者中关闭管道不会有任何效果，除非消费者也关闭了相同的管道端点。

管道实例化之后，形成2端。默认情况下，管道是双向的，左边send，右边recv，一端send和recv，会阻塞。它不是TCP和UDP，它是一台机器的多个进程。



```python
from multiprocessing import Pipe, Process


def func(conn):
    while True:
        msg = conn.recv()
        if msg is None: break
        print(conn.recv())


if __name__ == '__main__':
    conn1, conn2 = Pipe()
    Process(target=func, args=(conn1,)).start()
    for i in range(20):
        conn2.send('吃了么')
    conn2.send(None)
```

```python
from multiprocessing import Pipe, Process


def func(conn1, conn2):
    conn2.close()
    while True:
        try:
            msg = conn1.recv()
            print(msg)
        except EOFError:
            conn1.close()
            break


if __name__ == '__main__':
    conn1, conn2 = Pipe()
    Process(target=func, args=(conn1, conn2)).start()
    conn1.close()
    for i in range(20):
        conn2.send('吃了么')
    conn2.close()
```

## pipe实现生产者消费者模型

```python
import time
import random
from multiprocessing import Process, Pipe


def producer(pipe1, pipe2, name, food):
    pipe1.close()
    for i in range(10):
        time.sleep(random.random())
        f = '{}{}{}'.format(name, food, i)
        print('\033[1;31m%s\033[0m' % f)
        pipe2.send(f)
    pipe2.close()


def consumer(pipe1, pipe2, name):
    pipe2.close()
    while True:
        try:
            f = pipe1.recv()
            print('\033[1;32m%s消费了%s\033[0m' % (name, f))
            time.sleep(random.random())
        except EOFError:
            pipe1.close()
            break


if __name__ == '__main__':
    pipe1, pipe2 = Pipe()
    p = Process(target=producer, args=(pipe1, pipe2, 'alex', '泔水'))
    xxx = Process(target=consumer, args=(pipe1, pipe2, 'egon'))
    p.start()
    xxx.start()
    pipe1.close()
    pipe2.close()
```

**pipe数据不安全性**——加锁来控制操作管道的行为，来避免进程之间争抢数据造成的数据不安全现象

## 进程之间的数据共享

进程间数据是独立的，可以借助于队列或管道实现通信，二者都是基于消息传递的。虽然进程间数据独立，但可以通过Manager实现数据共享，事实上Manager的功能远不止于此。

Manager是一种较为高级的多进程通信方式，它能支持Python支持的的任何数据结构。它的原理是先启动一个ManagerServer进程，这个进程是阻塞的，它监听一个socket，然后其他进程（ManagerClient）通过socket来连接到ManagerServer，实现通信。

**修改字典的值**

```python
from multiprocessing import Manager, Process


def func(dic):
    dic['count'] = dic['count'] - 1
    print(dic)


if __name__ == '__main__':
    m = Manager()  # 创建一个server进程
    dic = m.dict({'count': 100})  # 这是一个特殊的字典
    p = Process(target=func, args=[dic, ])
    p.start()
    p.join()
    print(dic)

# {'count': 99}
# {'count': 99}
```

**循环修改**

```python
from multiprocessing import Manager, Process


def func(dic):
    dic['count'] -= 1  # 每次减1


if __name__ == '__main__':
    m = Manager()  # 创建一个server进程
    dic = m.dict({'count': 100})  # 这是一个特殊的字典
    p_lst = []  # 定义一个空列表
    for i in range(100):  # 启动100个进程
        p = Process(target=func, args=(dic,))
        p_lst.append(p)  # 进程追加到列表中
        p.start()  # 启动进程
    for p in p_lst: p.join()  # 等待100个进程全部结束
    print(dic)  # 打印dic的值
```

重复执行5次：{'count': 2}{'count': 0}{'count': 2}{'count': 3}{'count': 1}

因为同一个时间内有多个进程操作dic，就会发生数据错乱，为了解决这个问题，需要加锁。

```python
from multiprocessing import Manager, Process, Lock


def func(dic, lock):
    lock.acquire()  # 取得锁
    dic['count'] -= 1  # 每次减1
    lock.release()  # 释放锁


if __name__ == '__main__':
    m = Manager()  # 创建一个server进程
    lock = Lock()  # 创建锁
    dic = m.dict({'count': 100})  # 这是一个特殊的字典
    p_lst = []  # 定义一个空列表
    for i in range(100):  # 启动100个进程
        p = Process(target=func, args=(dic, lock))
        p_lst.append(p)  # 进程追加到列表中
        p.start()  # 启动进程
    for p in p_lst: p.join()  # 等待100个进程全部结束
    print(dic)
# {'count': 0}
```

**使用上下文管理**

```python
from multiprocessing import Manager, Process, Lock


def func(dic, lock):
    with lock:
        dic['count'] -= 1  # 每次减1


if __name__ == '__main__':
    m = Manager()
    lock = Lock()
    dic = m.dict({'count': 100})
    p_lst = []
    for i in range(100):  # 启动100个进程
        p = Process(target=func, args=(dic, lock))
        p_lst.append(p)  # 进程追加到列表中
        p.start()  # 启动进程
    for p in p_lst: p.join()  # 等待100个进程全部结束
    print(dic)  # 打印dic的值
```

之前学到的文件管理，有用到上下文管理。这里也可以使用上下文管理。有2个必要条件

1. 提供了with方法。
2. 必须有一个开始和结束动作。这里的开始和结束动作，分别指的是acquire和release。

## 进程池

为什么要有进程池?
在程序实际处理问题过程中，忙时会有成千上万的任务需要被执行，闲时可能只有零星任务。那么在成千上万个任务需要被执行的时候，我们就需要去创建成千上万个进程么？首先，创建进程需要消耗时间，销毁进程也需要消耗时间。第二即便开启了成千上万的进程，操作系统也不能让他们同时执行，这样反而会影响程序的效率。因此我们不能无限制的根据任务开启或者结束进程。那么我们要怎么做呢？

在这里，要给大家介绍一个进程池的概念，定义一个池子，在里面放上固定数量的进程，有需求来了，就拿一个池中的进程来处理任务，等到处理完毕，进程并不关闭，而是将进程再放回进程池中继续等待任务。如果有很多任务需要执行，池中的进程数量不够，任务就要等待之前的进程执行任务完毕归来，拿到空闲进程才能继续执行。也就是说，池中进程的数量是固定的，那么同一时间最多有固定数量的进程在运行。这样不会增加操作系统的调度难度，还节省了开闭进程的时间，也一定程度上能够实现并发效果。

![](images\0.png)



## Pool模块

**概念介绍**

```
Pool([numprocess  [,initializer [, initargs]]]):创建进程池
1 numprocess:要创建的进程数，如果省略，将默认使用cpu_count()的值
2 initializer：是每个工作进程启动时要执行的可调用对象，默认为None
3 initargs：是要传给initializer的参数组
```

**主要方法**

```
p.apply(func[, args[, kwargs]])
在一个池工作进程中执行func(*args, **kwargs), 然后返回结果。需要强调的是：此操作并不会在所有池工作进程中并发执行func函数。如果要通过不同参数并发地执行func函数，必须从不同线程调用p.apply()函数或者使用p.apply_async()。

p.apply_async(func[, args[, kwargs]])
在一个池工作进程中执行func(*args, **kwargs), 然后返回结果。
此方法的结果是AsyncResult类的实例，callback是可调用对象，接收输入参数。当func的结果变为可用时，传递给callback。callback禁止执行任何阻塞操作，否则将接收其他异步操作中的结果。

p.close()：关闭进程池，防止进一步操作。如果所有操作持续挂起，它们将在工作进程终止前完成。
P.jon()：等待所有工作进程退出。此方法只能在close（）或teminate()之后调用。
```

**其他方法(了解)**

```
方法apply_async()和map_async（）的返回值是AsyncResul的实例obj。
实例具有以下方法
obj.get()：返回结果，如果有必要则等待结果到达。timeout是可选的。如果在指定时间内还没有到达，将引发异常。如果远程操作中引发了异常，它将在调用此方法时再次被引发。
obj.ready()：如果调用完成，返回True。
obj.successful()：如果调用完成且没有引发异常，返回True，如果在结果就绪之前调用此方法，引发异常。
obj.wait([timeout]):等待结果变为可用。
obj.terminate()：立即终止所有工作进程，同时不执行任何清理或结束任何挂起工作。如果p被垃圾回收，将自动调用此函数。
```

**代码实例**

```python
import time
import os
from multiprocessing import Pool


def fc(i):
    time.sleep(0.5)
    print('func%s' % i, os.getpid())


if __name__ == '__main__':
    p = Pool(5)
    for i in range(5):
        p.apply(func=fc, args=(i,))  # 同步调用

# func0 2000
# func1 11064
# func2 7984
# func3 6472
# func4 11620
```

**进程池的同步调用**

```python
import os
import time
from multiprocessing import Pool


def work(n):
    print('%s run' % os.getpid())
    time.sleep(1)
    return n ** 2


if __name__ == '__main__':
    p = Pool(3)  # 进程池中从无到有创建三个进程,以后一直是这三个进程在执行任务

    for i in range(10):
        res = p.apply(work, args=(i,))  # 同步调用，直到本次任务执行完毕拿到res，等待任务work执行的
        # 过程中可能有阻塞也可能没有阻塞
        # 但不管该任务是否存在阻塞，同步调用都会在原地等着
        print(res)
```

**进程池的异步调用**

```python
import os
import time
import random
from multiprocessing import Pool


def work(n):
    print('%s run' % os.getpid())
    time.sleep(random.random())
    return n ** 2


if __name__ == '__main__':
    p = Pool(3)  # 进程池中从无到有创建三个进程,以后一直是这三个进程在执行任务
    res_l = []
    for i in range(10):
        res = p.apply_async(work, args=(i,))
        res_l.append(res)
    # 异步apply_async用法：如果使用异步提交的任务，主进程需要使用join，等待进程池内任务都处理完，然后可		以用get收集结果
    # 否则，主进程结束，进程池可能还没来得及执行，也就跟着一起结束了
    p.close()
    p.join()
    for res in res_l:
        print(res.get())  # 使用get来获取apply_aync的结果,如果是apply,则没有get方法,因为apply是同步执行,立刻获取结果,也根本无需get
```

## **没有返回值的情况**

```python
import time
from multiprocessing import Pool


def wahaha(i):
    time.sleep(1)
    print('*' * i)


if __name__ == '__main__':
    p = Pool(5)  # 建议的数量是CPU核数+1
    for i in range(5):
        p.apply_async(func=wahaha, args=(i,))
    p.close()  # 不能再提交新的任务
    p.join()  # 等待池中的任务都执行完


#
# *
# **
# ***
# ****

```

```python
from multiprocessing import Process
import time


def f(n):
    time.sleep(1)
    print('*' * n)


if __name__ == '__main__':
    for i in range(5):
        Process(target=f, args=(i,)).start()

# *
# ***
# **
#
# ****

```

执行输出是乱的，而进程池中不是乱的。

## 有返回值的情况

```python
import time
from multiprocessing import Pool


def wahaha(i):
    time.sleep(1)
    return '*' * i


if __name__ == '__main__':
    p = Pool(5)  # 建议的数量是CPU核数+1
    res_1 = []
    for i in range(1,5):
        res = p.apply_async(func=wahaha, args=(i,))
        res_1.append(res)
    for i in res_1: print(i.get())

# *
# **
# ***
# ****
```

## 进程池版socket并发聊天

server

```python
from socket import *
from multiprocessing import Pool
import os


def talk(conn):
    print('进程pid: %s' % os.getpid())
    while True:
        try:
            msg = conn.recv(1024)
            if not msg: break
            conn.send(msg.upper())
        except Exception as e:
            print(e)
            break


if __name__ == '__main__':
    server = socket(AF_INET, SOCK_STREAM)
    server.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)
    server.bind(('127.0.0.1', 8080))
    server.listen(5)
    p = Pool(4)
    while True:
        conn, adr = server.accept()
        p.apply_async(talk, args=(conn,))
```

client

```python
from socket import *

client = socket(AF_INET, SOCK_STREAM)
client.connect(('127.0.0.1', 8080))

while True:
    msg = input('>>').strip()
    if not msg: continue
    client.send(msg.encode('utf-8'))
    msg = client.recv(1024)
    print(msg.decode('utf-8'))
```

## map

它不能获取返回值，参数必须是一个可迭代对象。如果不需要返回值，使用map即可。

```python
import time
from multiprocessing import Pool


def wahaha(i):
    time.sleep(1)
    print('*' * i)


if __name__ == '__main__':
    p = Pool(5)  # 建议的数量是CPU核数+1
    p.map(func=wahaha, iterable=range(1, 5))  # iterable接收一个可迭代对象

# *
# **
# ***
# ****
```

**效率对比**

```python
import time
from multiprocessing import Pool, Process


def func(n):
    pass
    # for i in range(10):
    #     print((n + 1))


if __name__ == '__main__':
    t1 = time.time()
    pool = Pool(5)
    pool.map(func, range(100))
    t2 = time.time()
    p_lst = []
    for i in range(100):
        p = Process(target=func, args=(i,))
        p.start()
        p_lst.append(p)
    for p in p_lst: p.join()
    t3 = time.time()
    print(t2 - t1, t3 - t2)

# 0.16640734672546387 3.1360883712768555
```

```python
import time
from multiprocessing import Pool


def func(i):
    time.sleep(0.5)
    return i * i


if __name__ == '__main__':
    p = Pool(5)
    res = p.map(func, range(10))
    print(res)

# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
```

## 回调函数

回调函数在什么时候执行？子进程的任务执行完毕之后立即触发。
回调函数的参数：子进程的返回值。
回调函数是由谁执行的：主进程执行的。

在哪儿用？爬虫 ：如果要爬取多个格式相同的网页，真正影响程序效率的是网络的延迟。计算、分析、处理网页的时间是很快的。如果主进程要处理很久，那么就不适合用回调函数。

```python
import os
from multiprocessing import Pool


def func1(n):
    print('in func1', os.getpid())
    return n * n


def func2(nn):
    print('in func2', os.getpid())
    print(nn)


if __name__ == '__main__':
    print('主进程:', os.getpid())
    p = Pool(5)
    p.apply_async(func1, args=(10,), callback=func2)
    p.close()
    p.join()

# 主进程: 14244
# in func1 14440
# in func2 14244
# 100
```

如果在主进程中等待进程池中所有任务都执行完毕后，再统一处理结果，则无需回调函数。

```python
from multiprocessing import Pool
import time
import random
import os


def work(n):
    time.sleep(1)
    print(os.getpid())
    return n ** 2


if __name__ == '__main__':
    p = Pool()
    res_l = []
    for i in range(10):
        res = p.apply_async(work, args=(i,))
        res_l.append(res)
    p.close()
    p.join()  # 等待进程池中所有进程执行完毕
    nums = []
    for res in res_l:
        nums.append(res.get())  # 拿到所有结果
    print(nums)  # 主进程拿到所有的处理结果,可以在主进程中进行统一进行处理
```

## 其余

```
进程池的其他实现方式：https://docs.python.org/dev/library/concurrent.futures.html
参考资料
http://www.cnblogs.com/linhaifeng/articles/6817679.html
https://www.jianshu.com/p/1200fd49b583
https://www.jianshu.com/p/aed6067eeac
http://www.cnblogs.com/Eva-J/articles/8253549.html
```

